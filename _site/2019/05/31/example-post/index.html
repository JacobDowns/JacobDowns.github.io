<!DOCTYPE html> <html lang="en-us"> <head> <link href="http://gmpg.org/xfn/11" rel="profile"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta http-equiv="content-type" content="text/html; charset=utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1"> <title> Stuff &middot; J. Z. Downs </title> <link rel="stylesheet" href="/public/css/poole.css"> <link rel="stylesheet" href="/public/css/syntax.css"> <link rel="stylesheet" href="/public/css/lanyon.css"> <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400"> <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png"> <link rel="shortcut icon" href="/public/favicon.ico"> <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml"> <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  processEscapes: true},
  TeX: {
  extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
  equationNumbers: {
  autoNumber: "AMS"
  }
  }
  });
  </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML" type="text/javascript"></script> </head> <body> <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox"> <div class="sidebar" id="sidebar"> <div class="sidebar-item"> <p>A place for code projects, math, and visualizations.</p> </div> <nav class="sidebar-nav"> <a class="sidebar-nav-item" href="/">Home</a> <a class="sidebar-nav-item" href="/about/">About</a> <a class="sidebar-nav-item" href="/archive/v1.0.0.zip">Download</a> <a class="sidebar-nav-item" href="">GitHub project</a> <span class="sidebar-nav-item">Currently v1.0.0</span> </nav> <div class="sidebar-item"> <p> &copy; 2019. All rights reserved. </p> </div> </div> <div class="wrap"> <div class="masthead"> <div class="container"> <h3 class="masthead-title"> <a href="/" title="Home">J. Z. Downs</a> <small>Computational Mathematician</small> </h3> </div> </div> <div class="container content"> <div class="post"> <h1 class="post-title">Stuff</h1> <span class="post-date">31 May 2019</span> <h1 id="nonlinear-transformation-of-a-gaussian">Nonlinear Transformation of a Gaussian</h1> <p>Suppose that $\pmb{x} \sim N(\pmb{x_0}, P_x)$ is a Gaussian random variable with mean $\pmb{x_0}$ and covariance matrix $P_x$. If $f : \mathbb{R}^n \to \mathbb{R}^m$ is a nonlinear function, we would like to approximate the statistics of the non-Gaussian random variable \begin{equation} \pmb{y} = f(\pmb{x}) \end{equation} There are many practical applications of this problem, particularly in Gaussian filters such as the unscented Kalman problem. Formally, the probability density of the random variable $\pmb{y}$ is given by \begin{equation} P(\pmb{y}) = \begin{cases} |J(\pmb{y})| N(f^{-1}(\pmb{y}) | \pmb{x_0}, P_x) &amp; \text{ if } \pmb{y} = f(\pmb{x}) \text{ for some } \pmb{x} \\<br/> 0 &amp; \text{otherwise} \end{cases} \end{equation} where $|J(\pmb{y})|$ is the determinant of the Jacobian of $f^{-1}$. Technically this applies for strictly monotone differentiable functions $f$ \cite{Sarkka2013}.</p> <p>Below, we show a simple example of computing the PDF of a transformed Gaussian random variable analytically and via random sampling. In particular, we let $x \sim N(0, 1)$ and $f$ be the logistic function \begin{equation} y = \mathcal{F}(x) = \frac{1}{1 + e^{-x}}. \end{equation}</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">plot.basic_plot</span>

<span class="c1"># Plot the probability distribution for y = f(x) 
# where f(x) is the logistic function and x ~ N(0,1)
</span>
<span class="c1"># Probability density of x
</span><span class="k">def</span> <span class="nf">Px</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">)</span>

<span class="c1"># Nonlinear function
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Inverse of nonlinear function
</span><span class="k">def</span> <span class="nf">f_inv</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">x</span><span class="p">))</span>

<span class="c1"># Probability density of y
</span><span class="k">def</span> <span class="nf">Py</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="c1"># Determinant of Jacobian of F^{-1}(y) 
</span>    <span class="n">Jy</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Jy</span><span class="o">*</span><span class="n">Px</span><span class="p">(</span><span class="n">f_inv</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="c1"># Randomly sample from the distribution and plot a histogram 
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">7500</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">density</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="c1"># Plot the distribution computed analytically
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1e-16</span><span class="p">,</span> <span class="mf">1.0</span><span class="o">-</span><span class="mf">1e-16</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Py</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'P(y)'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div> <p><img src="/assets/images/gaussian_transformation_files/gaussian_transformation_1_0.png" alt="png"/></p> <h2 id="expected-value-integrals">Expected Value Integrals</h2> <p>Using the definition above is cumbersome and rarely practical for complicated nonlinear transformations. Typically, we are more interested in computing certain statistics of the random variable $\pmb{y}$, such as its mean and covariance. Enter the law of the unconscious statistician.</p> <p>As before, suppose that $\pmb{x}$ is a Gaussian random variable. We can compute the expected value or mean of $\pmb{y} = f(\pmb{x})$ denoted $E[\pmb{y}]$ without explicitly knowing its associated probability density function as follows</p> <p>\begin{equation} \label{eq:gwint} E[\pmb{y}] = \int_{\mathbb{R}^n} f(\pmb{x}) N(\pmb{x} | \pmb{x_0}, P_x) d \pmb{x}. \end{equation}</p> <p>That is, $E[\pmb{y}]$ can be computed as a Gaussian weighted integral. Let’s return to our logistic function example $y = f(x) = \frac{1}{1 + e^{-x}}$ and compute the expected value of $y$ using random sampling and numerical integration using the law of the unconscious statistician.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.integrate</span> <span class="kn">import</span> <span class="n">quad</span>

<span class="c1"># Estimate expected value of y by random sampling
</span><span class="n">y_mean1</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Estimate expected value of y using the result above 
# and numerical quadrature 
</span><span class="n">y_mean2</span> <span class="o">=</span> <span class="n">quad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">Px</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="o">-</span><span class="mf">6.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Random sampling estimate: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">y_mean1</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Integral estimate: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">y_mean2</span><span class="p">))</span>
</code></pre></div></div> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Random sampling estimate: 0.5014504701239816
Integral estimate: 0.49999999901341236
</code></pre></div></div> <p>Covariance is also defined in terms of expected values integrals, and can therefore be approximated using similar weighted integrals. We will discuss covariance of a transformed random variable in more detail later. \begin{equation} \text{Cov}[\pmb{y}] = E[(f(\pmb{x}) - E[f(\pmb{x})])(f(\pmb{x}) - E[f(\pmb{x})])^T] \end{equation}</p> <p>Given the importance of the expected value integrals in filtering applications, considerable effort has gone into efficiently estimating Gaussian weighted integrals of the form shown in Equation \ref{eq:gwint}. For a simple 1D problem, a basic quadrature rule suffices. However, for high dimensional problems, the number of points in standard quadrature rules grows exponentially, and estimating expected value integrals soon becomes intractable. In the next section, we’ll show an example of an efficient method for computing Gaussian weighted integrals called the Unscented Transform.</p> </div> <div class="related"> <h2>Related Posts</h2> <ul class="related-posts"> <li> <h3> <a href="/2014/01/02/introducing-lanyon/"> Math and Stuff <small>02 Jan 2014</small> </a> </h3> </li> <li> <h3> <a href="/2014/01/01/example-content/"> Example content <small>01 Jan 2014</small> </a> </h3> </li> <li> <h3> <a href="/2013/12/31/whats-jekyll/"> What's Jekyll? <small>31 Dec 2013</small> </a> </h3> </li> </ul> </div> </div> </div> <label for="sidebar-checkbox" class="sidebar-toggle"></label> <script>(function(b){var a=b.querySelector(".sidebar-toggle");var d=b.querySelector("#sidebar");var c=b.querySelector("#sidebar-checkbox");b.addEventListener("click",function(g){var f=g.target;if(!c.checked||d.contains(f)||(f===c||f===a)){return}c.checked=false},false)})(document);</script> </body> </html>