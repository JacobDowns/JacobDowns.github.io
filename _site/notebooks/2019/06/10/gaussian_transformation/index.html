<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Nonlinear Transformation of a Gaussian &middot; J. Z. Downs
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  processEscapes: true},
  TeX: {
  extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
  equationNumbers: {
  autoNumber: "AMS"
  }
  }
  });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML" type="text/javascript"></script>
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>A place code, math, and visualizations.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
      
    

    <span class="sidebar-nav-item">Currently v1.0.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2019. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">J. Z. Downs</a>
            <small>Sums and summits</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Nonlinear Transformation of a Gaussian</h1>
  <span class="post-date">10 Jun 2019</span>
  <p>Suppose that $\pmb{x} \sim N(\pmb{x_0}, P_x)$ is a Gaussian random variable with mean $\pmb{x_0}$ and covariance matrix $P_x$. If $f : \mathbb{R}^n \to \mathbb{R}^m$ is a nonlinear function, we would like to approximate the statistics of the non-Gaussian random variable
\begin{equation}
\pmb{y} = f(\pmb{x}) 
\end{equation}
There are many practical applications of this problem, particularly in Gaussian filters such as the unscented Kalman problem. Formally, the probability density of the random variable $\pmb{y}$ is given by 
\begin{equation}
P(\pmb{y}) = 
\begin{cases} 
      |J(\pmb{y})| N(f^{-1}(\pmb{y}) | \pmb{x_0}, P_x) &amp; \text{ if } \pmb{y} = f(\pmb{x}) \text{ for some } \pmb{x} <br />
      0 &amp; \text{otherwise} 
\end{cases}
\end{equation}
where $|J(\pmb{y})|$ is the determinant of the Jacobian of $f^{-1}$. Technically this applies for strictly monotone differentiable functions $f$ <a class="citation" href="#Sarkka2013">(Sarkka, 2013)</a>.</p>

<p>Below, we show a simple example of computing the PDF of a transformed Gaussian random variable analytically and via random sampling. In particular, we let $x \sim N(0, 1)$ and $f$ be the logistic function 
\begin{equation}
y = \mathcal{F}(x) = \frac{1}{1 + e^{-x}}.
\end{equation}</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s">'font.size'</span><span class="p">:</span> <span class="mi">22</span><span class="p">})</span>

<span class="c1"># Plot the probability distribution for y = f(x) 
# where f(x) is the logistic function and x ~ N(0,1)
</span>
<span class="c1"># Probability density of x
</span><span class="k">def</span> <span class="nf">Px</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">)</span>

<span class="c1"># Nonlinear function
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Inverse of nonlinear function
</span><span class="k">def</span> <span class="nf">f_inv</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">x</span><span class="p">))</span>

<span class="c1"># Probability density of y
</span><span class="k">def</span> <span class="nf">Py</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="c1"># Determinant of Jacobian of F^{-1}(y) 
</span>    <span class="n">Jy</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Jy</span><span class="o">*</span><span class="n">Px</span><span class="p">(</span><span class="n">f_inv</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="c1"># Randomly sample from the distribution and plot a histogram 
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">density</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="c1"># Plot the distribution computed analytically
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1e-16</span><span class="p">,</span> <span class="mf">1.0</span><span class="o">-</span><span class="mf">1e-16</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Py</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'P(y)'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/assets/images/gaussian_transformation_files/gaussian_transformation_1_0.png" alt="png" /></p>

<h2 id="expected-value-integrals">Expected Value Integrals</h2>

<p>Using the definition above is cumbersome and rarely practical for complicated nonlinear transformations. Typically, we are more interested in computing certain statistics of the random variable $\pmb{y}$, such as its mean and covariance. Enter the law of the unconscious statistician.</p>

<p>As before, suppose that $\pmb{x}$ is a Gaussian random variable. We can compute the expected value or mean of $\pmb{y} = f(\pmb{x})$ denoted $E[\pmb{y}]$ without explicitly knowing its associated probability density function as follows</p>

<p>\begin{equation}
\label{eq:gwint}
E[\pmb{y}] = \int_{\mathbb{R}^n} f(\pmb{x}) N(\pmb{x} | \pmb{x_0}, P_x) d \pmb{x}.
\end{equation}</p>

<p>That is, $E[\pmb{y}]$ can be computed as a Gaussian weighted integral. Let’s return to our logistic function example $y = f(x) = \frac{1}{1 + e^{-x}}$ and compute the expected value of $y$ using random sampling and numerical integration using the law of the unconscious statistician.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.integrate</span> <span class="kn">import</span> <span class="n">quad</span>

<span class="c1"># Estimate expected value of y by random sampling
</span><span class="n">y_mean1</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Estimate expected value of y using the result above 
# and numerical quadrature 
</span><span class="n">y_mean2</span> <span class="o">=</span> <span class="n">quad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">Px</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="o">-</span><span class="mf">6.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Random sampling estimate: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">y_mean1</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Integral estimate: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">y_mean2</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Random sampling estimate: 0.5014504701239816
Integral estimate: 0.49999999901341236
</code></pre></div></div>

<p>Given the importance of expected value integrals in filtering applications, considerable effort has gone into efficiently estimating Gaussian weighted integrals of the form shown in Equation \ref{eq:gwint}. For a simple 1D problem, a basic quadrature rule suffices. However, generalizations of 1D quadrature rules to many dimensions result in methods in which the number of points grows exponentially with dimension. Hence, these methods are computationally intractable for many problems. In the next section, we’ll show an example of a much more efficient method for computing Gaussian weighted integrals called the Unscented Transform.</p>

<h2 id="bibliography">Bibliography</h2>

<ol class="bibliography"><li><span id="Sarkka2013">Sarkka, S. (2013). Bayesian Filtering and Smoothing. <i>Cambridge University Press</i>. https://doi.org/10.1017/CBO9781139344203</span></li></ol>

</div>

<div class="relatedPosts">

<h4>Related Posts</h4>







    
    

    

    
      <div>
      <h5><a href="/notebooks/2019/06/11/sigmapy/">Sigmapy</a></h5>
      </div>
      
      
    

  

    
    

    

    
      <div>
      <h5><a href="/notebooks/2019/06/10/sigma_point_sets/">Sigma Point Sets</a></h5>
      </div>
      
      
    

  

    
    

    

    

  

    
    

    

    
      <div>
      <h5><a href="/notebooks/2019/06/10/gaussian_quadrature/">Gaussian Quadrature</a></h5>
      </div>
      
      
    

  

    
    

    

    
      <div>
      <h5><a href="/notebooks/2019/06/10/conditional_distributions/">Conditional Distributions</a></h5>
      </div>
      
      
        

</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
