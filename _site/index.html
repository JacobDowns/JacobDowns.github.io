<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      J. Z. Downs &middot; Sums and summits
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  processEscapes: true},
  TeX: {
  extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
  equationNumbers: {
  autoNumber: "AMS"
  }
  }
  });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML" type="text/javascript"></script>
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>A place code, math, and visualizations.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
      
    

    <span class="sidebar-nav-item">Currently v1.0.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2019. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">J. Z. Downs</a>
            <small>Sums and summits</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/notebooks/2019/06/11/sigmapy/">
        Sigmapy
      </a>
    </h1>

    <span class="post-date">11 Jun 2019</span>

    <p>Sigmapy is a Python package for computing sigma point and weight sets
for estimating Gaussian weighted integrals common in Gaussian filtering
applications like the Unscented Kalman filter. Installation instructions
and documentation is available <a href="/sigmapy/">here</a>.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/notebooks/2019/06/10/sigma_point_sets/">
        Sigma Point Sets
      </a>
    </h1>

    <span class="post-date">10 Jun 2019</span>

    <p>Here we outline algorithms for generating each sigma point set in Sigmapy for a standard Gaussian weighting function, $\mathcal{N}(\pmb{0}, I)$ in $n$ dimensions with mean $\pmb{0}$ and identity covariance. Sigma points are denoted $\chi_i$, mean weights $w_i^m$, and covariance weights $w_i^c$. General Gaussian weighting functions are $\mathcal{N}(\pmb{x_0}, P_x)$ are handled by transforming each sigma points $\chi_i$ according to 
\begin{equation}
\pmb{\chi_i}’ = \sqrt{P_x} \pmb{\chi_i} + \pmb{x_0}
\end{equation}
where $\sqrt{P_x}$ is a matrix square root of $P_x$. The following sigma point sets are available Sigmapy.</p>

<table style="width:100%">
  <tr>
    <th>Shorthand Name</th>
    <th>Number of Points</th> 
    <th>Order</th>
  </tr>
  <tr>
    <td>merwe</td>
    <td>$2n+1$</td> 
    <td>3</td>
  </tr>
  <tr>
    <td>julier</td>
    <td>$2n+1$</td> 
    <td>3</td>
  </tr>
  <tr>
    <td>menegaz</td>
    <td>$n+1$</td> 
    <td>2</td>
  </tr>
  <tr>
    <td>simplex</td>
    <td>$n+1$</td> 
    <td>2</td>
  </tr>
  <tr>
    <td>gauss_hermite</td>
    <td>$3^n$</td> 
    <td>3</td>
  </tr>
  <tr>
    <td>li</td>
    <td>$2n^2 + 1$</td> 
    <td>5</td>
  </tr>
    <tr>
    <td>mysovskikh</td>
    <td>$n^2 + 3n + 3$</td> 
    <td>5</td>
  </tr>
</table>

<h2 id="van-der-merwe-sigma-points">Van der Merwe Sigma Points</h2>

<p>A commonly used set of points and weights for the unscented transform is the Van der Merwe set <a class="citation" href="#VanderMerwe2004">(Van der Merwe, 2004)</a>. It uses $2n + 1$ sigma points given by
\begin{equation}
\pmb{\chi_i} =
\begin{cases} 
      \pmb{x_0} &amp; i = 0 \\
      \pmb{x_0} - \sqrt{n + \lambda} \pmb{e_i} &amp; i = 1, \cdots, n\\
      \pmb{x_0} + \sqrt{n + \lambda} \pmb{e_i} &amp; i = n+1, \cdots, 2n
\end{cases}
\end{equation}
where
\begin{equation}
\lambda = \alpha^2 (n + \kappa) - n.
\end{equation}
The notation $\pmb{e_i}$ refers to the $i$-th column of the $n \times n$ identity matrix. There are different weights $w_i^m$ and $w_i^c$ for computing mean and covariance estimates respectively
\begin{equation}
w_i^m =
\begin{cases} 
      \frac{\lambda}{n + \lambda} &amp; i = 0 \\
      \frac{1}{2(n+\lambda)} &amp; i = 1, \cdots, 2n
\end{cases}
\end{equation}</p>

<p>\begin{equation}
w_i^c =
\begin{cases} 
      \frac{\lambda}{n + \lambda} + 1 - \alpha^2 + \beta &amp; i = 0 \\
      \frac{1}{2(n+\lambda)} &amp; i = 1, \cdots, 2n
\end{cases}
\end{equation}</p>

<p>Appropriate default choices of the scaling parameters are, $\beta = 2$, $\kappa = 3 - n$, and $0 \leq \alpha \leq 1$. Unless otherwise specified, these are the values used by Sigmapy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sigmapy.sigma_sets</span> <span class="kn">import</span> <span class="n">SigmaSets</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s">'font.size'</span><span class="p">:</span> <span class="mi">18</span><span class="p">})</span>

<span class="c1"># Prior mean
</span><span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Prior covariance
</span><span class="n">Px</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Van der Merwe sigma points in 2D
</span><span class="n">sets</span> <span class="o">=</span> <span class="n">SigmaSets</span><span class="p">()</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">wc</span> <span class="o">=</span> <span class="n">sets</span><span class="o">.</span><span class="n">get_set</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">Px</span><span class="p">,</span> <span class="n">set_name</span> <span class="o">=</span> <span class="s">'merwe'</span><span class="p">)</span>
<span class="c1"># Different scaling parameters
</span><span class="n">X2</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">wc</span> <span class="o">=</span> <span class="n">sets</span><span class="o">.</span><span class="n">get_set</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">Px</span><span class="p">,</span> <span class="n">set_name</span> <span class="o">=</span> <span class="s">'merwe'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Van der Merwe Sigma Points'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X1</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">X1</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span> <span class="o">=</span> <span class="mf">250.</span><span class="o">*</span><span class="n">wm</span> <span class="o">/</span> <span class="n">wm</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X2</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">X2</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span> <span class="o">=</span> <span class="mf">250.</span><span class="o">*</span><span class="n">wm</span> <span class="o">/</span> <span class="n">wm</span><span class="o">.</span><span class="nb">max</span><span class="p">(),</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'^'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/assets/images/sigma_point_sets_files/sigma_point_sets_1_0.png" alt="png" /></p>

<h2 id="julier-sigma-points">Julier Sigma Points</h2>

<p>Another commonly used set of sigma points is the Julier set <a class="citation" href="#Julier1997">(Julier &amp; Uhlmann, 1997)</a>.  It has $2n+1$ points and a single scaling parameter $\kappa$. Setting $\kappa = 3 - n$ matches one higher order moment, and is therefore the preferred default value. However, the best choice of the scaling parameter is problem dependent. Points are given by
\begin{equation}
\pmb{\chi_i} =
\begin{cases} 
      \pmb{x_0} &amp; i = 0 \\
      \pmb{x_0} - \sqrt{n + \kappa} \pmb{e_i} &amp; i = 1, \cdots, n\\
      \pmb{x_0} + \sqrt{n + \kappa} \pmb{e_i} &amp; i = n+1, \cdots, 2n
\end{cases}
\end{equation}
and weights are defined by
\begin{equation}
w_i^m = w_i^c =
\begin{cases} 
      \frac{\kappa}{n + \kappa} &amp; i = 0 \\
      \frac{1}{2(n+\kappa)} &amp; i = 1, \cdots, 2n.
\end{cases}
\end{equation}</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X1</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">wc</span> <span class="o">=</span> <span class="n">sets</span><span class="o">.</span><span class="n">get_set</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">Px</span><span class="p">,</span> <span class="n">set_name</span> <span class="o">=</span> <span class="s">'julier'</span><span class="p">)</span>
<span class="c1"># Different scaling parameter
</span><span class="n">X2</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">wc</span> <span class="o">=</span> <span class="n">sets</span><span class="o">.</span><span class="n">get_set</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">Px</span><span class="p">,</span> <span class="n">set_name</span> <span class="o">=</span> <span class="s">'julier'</span><span class="p">,</span> <span class="n">kappa</span> <span class="o">=</span> <span class="mf">2.</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Julier Sigma Points'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X1</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">X1</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span> <span class="o">=</span> <span class="mf">250.</span><span class="o">*</span><span class="n">wm</span> <span class="o">/</span> <span class="n">wm</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X2</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">X2</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span> <span class="o">=</span> <span class="mf">250.</span><span class="o">*</span><span class="n">wm</span> <span class="o">/</span> <span class="n">wm</span><span class="o">.</span><span class="nb">max</span><span class="p">(),</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'^'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/sigma_point_sets_files/sigma_point_sets_3_0.png" alt="png" /></p>

<h2 id="menegaz-sigma-points">Menegaz Sigma Points</h2>

<p>The Menegaz set uses only $n+1$ sigma points <a class="citation" href="#Menegaz2011">(Menegaz, Ishihara, &amp; Borges, 2011)</a>. It has a single scaling parameter $0 &lt; w_0 &lt; 1$. Some additional notation is necessary to define the sigma point and weight sets. The notation $[c]_{p \times q}$ with $c \in \mathbb{R}$ represents a matrix of dimension $p \times q$ in which all terms are equal to $c$. The notation $\text{diag}(\pmb{x})$ for $\pmb{x} \in \mathbb{R}^n$ denotes an $n \times n$ matrix with $\pmb{x}$ on the diagonal. Using these definitions, we have:
\begin{equation}
X = \left [ \pmb{\chi_0} \cdots \pmb{\chi_n} \right ] = \left [- \frac{[\alpha]_{n \times 1}}{w_0} \; \; C (\sqrt{W})^{-1} \right ] + \left [ \pmb{x_0} \cdots \pmb{x_0} \right ]
\end{equation}</p>

<p>\begin{equation}
W = \text{diag}([w_1, \cdots, w_n])
\end{equation}</p>

<p>\begin{equation}
\begin{bmatrix} 
w_1 &amp; \cdots &amp; \sqrt{w_1}\sqrt{w_n} \\
\vdots &amp; \ddots &amp; \vdots \\
\sqrt{w_1} \sqrt{w_n} &amp; \cdots &amp; w_n 
\end{bmatrix}
= w_0 \alpha^2 C^{-1} [1]_{n \times n} (C^T)^{-1}
\end{equation}</p>

<p>\begin{equation}
C = \sqrt{I - \alpha^2 [1]_{n \times n}}
\end{equation}</p>

<p>\begin{equation}
\alpha = \sqrt{\frac{1 - w_0}{n}}
\end{equation}</p>

<p>Note that $w_i^m = w_i^c = w_i$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Menegaz points in 2D
</span><span class="n">X1</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">wc</span> <span class="o">=</span> <span class="n">sets</span><span class="o">.</span><span class="n">get_set</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">Px</span><span class="p">,</span> <span class="n">set_name</span> <span class="o">=</span> <span class="s">'menegaz'</span><span class="p">)</span>
<span class="c1"># Different scaling parameter
</span><span class="n">X2</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">wc</span> <span class="o">=</span> <span class="n">sets</span><span class="o">.</span><span class="n">get_set</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">Px</span><span class="p">,</span> <span class="n">set_name</span> <span class="o">=</span> <span class="s">'menegaz'</span><span class="p">,</span> <span class="n">w0</span> <span class="o">=</span> <span class="mf">.9</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Menegaz Sigma Points'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X1</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">X1</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span> <span class="o">=</span> <span class="mf">250.</span><span class="o">*</span><span class="n">wm</span> <span class="o">/</span> <span class="n">wm</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X2</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">X2</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span> <span class="o">=</span> <span class="mf">250.</span><span class="o">*</span><span class="n">wm</span> <span class="o">/</span> <span class="n">wm</span><span class="o">.</span><span class="nb">max</span><span class="p">(),</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'^'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/sigma_point_sets_files/sigma_point_sets_5_0.png" alt="png" /></p>

<h2 id="simplex-sigma-points">Simplex Sigma Points</h2>

<p>Another minimal set of $n+1$ sigma points, called the simplex set, is given in <a class="citation" href="#Moireau2011">(Moireau &amp; Chapelle, 2011)</a>. A matrix containing the sigma points as column vectors
\begin{equation}
X_{n+1}^* = \left [ \pmb{\chi_0} \; \cdots \; \pmb{\chi_n} \right ] 
\end{equation}
is generated recursively. We let 
\begin{equation}
X_1 = \left [-\frac{1}{2 \alpha} \; \frac{1}{2 \alpha} \right ]
\end{equation}
\begin{equation}
\alpha = \frac{n}{n+1}
\end{equation}
$X_i^{*}$ is generated from $X_{i-1}^*$ via
\begin{equation}
X_i^* = 
\begin{bmatrix} 
 &amp; &amp; &amp; 0 \\
 &amp; X_{i-1}^* &amp; &amp; \vdots \\
 &amp; &amp; &amp; 0 \\
 \frac{1}{\sqrt{\alpha i (i + 1)}} &amp;  \frac{1}{\sqrt{\alpha i (i + 1)}} &amp;  \frac{1}{\sqrt{\alpha i (i + 1)}} &amp;  \frac{-i}{\sqrt{\alpha i (i + 1)}}
\end{bmatrix}.
\end{equation}
Weights are given by 
\begin{equation}
w_i^m = w_i^c = \alpha.
\end{equation}.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplex points in 2D
</span><span class="n">X</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">wc</span> <span class="o">=</span> <span class="n">sets</span><span class="o">.</span><span class="n">get_set</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">Px</span><span class="p">,</span> <span class="n">set_name</span> <span class="o">=</span> <span class="s">'simplex'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Simplex Sigma Points'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span> <span class="o">=</span> <span class="mf">250.</span><span class="o">*</span><span class="n">wm</span> <span class="o">/</span> <span class="n">wm</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/sigma_point_sets_files/sigma_point_sets_7_0.png" alt="png" /></p>

<h2 id="li-sigma-points">Li Sigma Points</h2>

<p>Sigmapy includes a modified version of the fifth-order quadrature rule in <a class="citation" href="#Zhao-Ming2017">(Zhao-Ming, Wen-Ge, Dan, &amp; Yu-Rong, 2017)</a> . It contains one scaling parameter $\lambda_2$. The quadrature rule is formulated in terms of fully symmetric sets of points. A set of points $S = \{\pmb{s_1}, \pmb{s_2}, \cdots, \pmb{s_N}\}$ is fully symmetric if it is closed under the operations of coordinate position and sign permutations. The set $S$ is said to be generated by the vector $\pmb{s}$ if all points in $S$ can be obtained by coordinate position and sign permutations of $\pmb{s}$.</p>

<p>Sigma points are categorized in terms of three fully symmetric sets
\begin{equation}
\begin{gathered}
X_0 = [0]_n  \\
X_1 = [\lambda_1]_n \\
X_2 = [\lambda_2, \lambda_2]_n
\end{gathered}
\end{equation}
where, for example, the notation $[\lambda_2, \lambda_2]_n$ refers to the fully symmetric set of points generated by the $n$-dimensional vector $[\lambda_2, \lambda_2, 0, \cdots, 0]^T$. The set $X_0$ is trivial, containing only the zero vector. An example of the fully symmetric set $[1,1]_3$ is shown below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">combinations</span><span class="p">,</span> <span class="n">product</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">comb</span>

<span class="k">def</span> <span class="nf">generate_fully_symmetric_set</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">vals</span><span class="p">):</span>
    <span class="s">"""
    Generates a fully set of symmetric points of dimension n with 
    values given in vals. 

    Parameters
    ----------

    n: int
        Dimension of points in fully symmetric set

    vals : numpy.array(k)
        Non-zero values in the fully symmetric set

    Returns
    -------

    S : numpy.array(n, 2^k * (n choose k))
        Each column is a point in the fully symmetric set
    """</span>
     
    <span class="n">indexes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span>
    <span class="n">index_combs</span> <span class="o">=</span> <span class="n">combinations</span><span class="p">(</span><span class="n">indexes</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">k</span> <span class="o">*</span> <span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">,</span> <span class="n">exact</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)))</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">index_comb</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">index_combs</span><span class="p">):</span>
        <span class="n">signs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">k</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
        <span class="n">signs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span>
        <span class="n">sign_combs</span> <span class="o">=</span> <span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="n">signs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">sign_comb</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">sign_combs</span><span class="p">):</span>  
            <span class="n">S</span><span class="p">[</span><span class="n">index_comb</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">vals</span>
            <span class="n">S</span><span class="p">[</span><span class="n">index_comb</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="n">sign_comb</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            
    <span class="k">return</span> <span class="n">S</span>

<span class="k">print</span><span class="p">(</span><span class="n">generate_fully_symmetric_set</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[-1. -1.  1.  1. -1. -1.  1.  1.  0.  0.  0.  0.]
 [ 1. -1.  1. -1.  0.  0.  0.  0. -1. -1.  1.  1.]
 [ 0.  0.  0.  0.  1. -1.  1. -1.  1. -1.  1. -1.]]
</code></pre></div></div>

<p>The parameter $\lambda_1$ depends on $\lambda_2$ with 
\begin{equation}
\lambda_1 = \frac{\lambda_2 \sqrt{n-4}}{n - \lambda_2^2 - 1}.
\end{equation}
Hence, this method requires $n \geq 5$ and $n - \lambda_2^2 - 1 \neq 0$. Each sigma point in a category has the same weight. Moreover mean and covariance weights are the same. The sets $X_0$, $X_1$, and $X_2$ have associated weights $w_0$, $w_1$, and $w_2$ respectively where
\begin{equation}
w_2 = \frac{4-n}{2 \lambda_1^4}
\end{equation}
\begin{equation}
w_3 = \frac{1}{4 \lambda_1^4}
\end{equation}
\begin{equation}
w_1 = 1 - 2 n w_2 - 2n(n-1)w_3.
\end{equation}</p>

<h2 id="gauss-hermite-sigma-points">Gauss-Hermite Sigma Points</h2>

<p>The third order Gauss-Hermite quadrature rule is a generalization of the 1D Gauss-Hermite quadrature rule for integrating 
\begin{equation}
\int_{\mathbb{R}} f(x) e^{-x^2} \; dx.
\end{equation}
Weights and sigma points can be obtained using a tensor-products of the 1D Gauss-Hermite quadrature rule. The number of points grows exponentially with dimension, and the third order method requires $3^n$ points. An alternative formulation of the Gauss-Hermite sigma points involves categories of fully symmetric sets of sigma points <a class="citation" href="#Peng2017">(Peng, Duan, &amp; Zhu, 2017)</a>. In particular, there are $n+1$ classes of sigma points
\begin{equation}
\begin{gathered}
X_0 = \left [ 0 \right ]_n \\
X_1 = \left [ \sqrt{3} \right ]_n \\
X_2 = \left [ \sqrt{3}, \sqrt{3} \right ]_n \\
\vdots <br />
X_n = \left [\sqrt{3}, \cdots, \sqrt{3} \right ]_n.
\end{gathered}
\end{equation}
Each sigma point in a given fully symmetric category has the same weight:
\begin{equation}
\begin{gathered}
w_0 = \left ( \frac{2}{3} \right )^n \\
w_1 = \left ( \frac{2}{3} \right )^{n-1} \left ( \frac{1}{6} \right ) \\
w_2 = \left ( \frac{2}{3} \right )^{n-2} \left ( \frac{1}{6} \right )^2 \\
\vdots \\
w_n = \left ( \frac{1}{6} \right )^n.
\end{gathered}
\end{equation}</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Gauss-hermite sigma points in 2D
</span><span class="n">X</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">wc</span> <span class="o">=</span> <span class="n">sets</span><span class="o">.</span><span class="n">get_set</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">Px</span><span class="p">,</span> <span class="n">set_name</span> <span class="o">=</span> <span class="s">'hermite'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Gauss-Hermite Sigma Points'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span> <span class="o">=</span> <span class="mf">250.</span><span class="o">*</span><span class="n">wm</span> <span class="o">/</span> <span class="n">wm</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/assets/images/sigma_point_sets_files/sigma_point_sets_12_0.png" alt="png" /></p>

<h2 id="mysovskikh-sigma-points">Mysovskikh Sigma Points</h2>

<blockquote>
  <p>Note this method currently has a bug that I still need to fix.</p>
</blockquote>

<p>Another fifth order sigma point set is the Mysovskikh set, which has $n^2 + 3n + 3$ points <a class="citation" href="#Mysovskikh1970">(Mysovskikh, 1970)</a>. The quadrature rule can be obtained from the transformation group of the regular simplex with vertices $\pmb{a_1}, \cdots, \pmb{a_n}$ where $j$-th element of $\pmb{a_i}$ 
\begin{equation}
a_{i,j} = 
\begin{cases} 
      -\sqrt{ \frac{n+1}{n(n-j+2)(n-j+1)}} &amp; i &lt; j \\
      \sqrt{\frac{(n+1)(n-i+1)}{n(n-i+2)}} &amp; i = j \\
      0 &amp; i &gt; j
\end{cases}.
\end{equation}
The points $\pmb{a_i}$ are sigma points, as are the origin $\pmb{0}$, and points in the set
\begin{equation}
\{ \pmb{b_k} \} = \left \{ \sqrt{\frac{n}{2(n-1)}} \left ( \pmb{a_l} + \pmb{a_m} \right ) \; | \; l &lt; m, \; \; m = 1, 2, \cdots, n+1  \right \}. 
\end{equation}
The method is centrally symmetric. Hence, for every sigma point $\pmb{a_i}$ and $\pmb{b_k}$, -$\pmb{a_i}$ and -$\pmb{b_k}$ are also sigma points. The sigma point $\pmb{0}$ has associated weight $w_0$, while points $\pm \pmb{a_i}$ and $\pm \pmb{b_j}$ have associated weights $w_1$ and $w_2$ respectively</p>

<p>\begin{equation}
\begin{gathered}
w_0 = \frac{2}{n + 2} \\
w_1 = \frac{n^2 (7 - n)}{2(n+1)^2 (n+2)^2} \\
w_2 = \frac{2(n-1)^2}{(n+1)^2 (n+2)^2}.
\end{gathered}
\end{equation}</p>

<h2 id="bibliography">Bibliography</h2>

<ol class="bibliography"><li><span id="VanderMerwe2004">Van der Merwe, R. (2004). Sigma-point Kalman filters for probabilistic inference in dynamic state-space models. <i>PhD Thesis</i>. https://doi.org/10.6083/M4Z60KZ5</span></li>
<li><span id="Julier1997">Julier, S. J., &amp; Uhlmann, J. K. (1997). New extension of the Kalman filter to nonlinear systems. <i>Int Symp AerospaceDefense Sensing Simul and Controls</i>. https://doi.org/10.1117/12.280797</span></li>
<li><span id="Menegaz2011">Menegaz, H. M., Ishihara, J. Y., &amp; Borges, G. A. (2011). A new smallest sigma set for the Unscented Transform and its applications on SLAM. In <i>Proceedings of the IEEE Conference on Decision and Control</i>. https://doi.org/10.1109/CDC.2011.6161480</span></li>
<li><span id="Moireau2011">Moireau, P., &amp; Chapelle, D. (2011). Erratum of article “Reduced-order Unscented Kalman Filtering with application to parameter identification in large-dimensional systems.” <i>ESAIM: Control, Optimisation and Calculus of Variations</i>, <i>17</i>(2), 406–409. https://doi.org/10.1051/cocv/2011001</span></li>
<li><span id="Zhao-Ming2017">Zhao-Ming, L., Wen-Ge, Y., Dan, D., &amp; Yu-Rong, L. (2017). A novel algorithm of fifth-degree cubature Kalman filter for orbit determination at the lower bound approaching to the number of cubature points. <i>ACTA PHYSICA SINICA</i>. https://doi.org/10.7498/aps.66.158401</span></li>
<li><span id="Peng2017">Peng, L., Duan, X., &amp; Zhu, J. (2017). A New Sparse Gauss-Hermite Cubature Rule Based on Relative-Weight-Ratios for Bearing-Ranging Target Tracking. <i>Modelling and Simulation in Engineering</i>. https://doi.org/10.1155/2017/2783781</span></li>
<li><span id="Mysovskikh1970">Mysovskikh, I. P. (1970). Cubature formulae and orthogonal polynomials. <i>USSR Computational Mathematics and Mathematical Physics</i>. https://doi.org/10.1016/0041-5553(70)90159-X</span></li></ol>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/notebooks/2019/06/10/gaussian_transformation/">
        Nonlinear Transformation of a Gaussian
      </a>
    </h1>

    <span class="post-date">10 Jun 2019</span>

    <p>Suppose that $\pmb{x} \sim N(\pmb{x_0}, P_x)$ is a Gaussian random variable with mean $\pmb{x_0}$ and covariance matrix $P_x$. If $f : \mathbb{R}^n \to \mathbb{R}^m$ is a nonlinear function, we would like to approximate the statistics of the non-Gaussian random variable
\begin{equation}
\pmb{y} = f(\pmb{x}) 
\end{equation}
There are many practical applications of this problem, particularly in Gaussian filters such as the unscented Kalman problem. Formally, the probability density of the random variable $\pmb{y}$ is given by 
\begin{equation}
P(\pmb{y}) = 
\begin{cases} 
      |J(\pmb{y})| N(f^{-1}(\pmb{y}) | \pmb{x_0}, P_x) &amp; \text{ if } \pmb{y} = f(\pmb{x}) \text{ for some } \pmb{x} <br />
      0 &amp; \text{otherwise} 
\end{cases}
\end{equation}
where $|J(\pmb{y})|$ is the determinant of the Jacobian of $f^{-1}$. Technically this applies for strictly monotone differentiable functions $f$ <a class="citation" href="#Sarkka2013">(Sarkka, 2013)</a>.</p>

<p>Below, we show a simple example of computing the PDF of a transformed Gaussian random variable analytically and via random sampling. In particular, we let $x \sim N(0, 1)$ and $f$ be the logistic function 
\begin{equation}
y = \mathcal{F}(x) = \frac{1}{1 + e^{-x}}.
\end{equation}</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s">'font.size'</span><span class="p">:</span> <span class="mi">22</span><span class="p">})</span>

<span class="c1"># Plot the probability distribution for y = f(x) 
# where f(x) is the logistic function and x ~ N(0,1)
</span>
<span class="c1"># Probability density of x
</span><span class="k">def</span> <span class="nf">Px</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">)</span>

<span class="c1"># Nonlinear function
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Inverse of nonlinear function
</span><span class="k">def</span> <span class="nf">f_inv</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">x</span><span class="p">))</span>

<span class="c1"># Probability density of y
</span><span class="k">def</span> <span class="nf">Py</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="c1"># Determinant of Jacobian of F^{-1}(y) 
</span>    <span class="n">Jy</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Jy</span><span class="o">*</span><span class="n">Px</span><span class="p">(</span><span class="n">f_inv</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="c1"># Randomly sample from the distribution and plot a histogram 
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">density</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="c1"># Plot the distribution computed analytically
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1e-16</span><span class="p">,</span> <span class="mf">1.0</span><span class="o">-</span><span class="mf">1e-16</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Py</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'P(y)'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/assets/images/gaussian_transformation_files/gaussian_transformation_1_0.png" alt="png" /></p>

<h2 id="expected-value-integrals">Expected Value Integrals</h2>

<p>Using the definition above is cumbersome and rarely practical for complicated nonlinear transformations. Typically, we are more interested in computing certain statistics of the random variable $\pmb{y}$, such as its mean and covariance. Enter the law of the unconscious statistician.</p>

<p>As before, suppose that $\pmb{x}$ is a Gaussian random variable. We can compute the expected value or mean of $\pmb{y} = f(\pmb{x})$ denoted $E[\pmb{y}]$ without explicitly knowing its associated probability density function as follows</p>

<p>\begin{equation}
\label{eq:gwint}
E[\pmb{y}] = \int_{\mathbb{R}^n} f(\pmb{x}) N(\pmb{x} | \pmb{x_0}, P_x) d \pmb{x}.
\end{equation}</p>

<p>That is, $E[\pmb{y}]$ can be computed as a Gaussian weighted integral. Let’s return to our logistic function example $y = f(x) = \frac{1}{1 + e^{-x}}$ and compute the expected value of $y$ using random sampling and numerical integration using the law of the unconscious statistician.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.integrate</span> <span class="kn">import</span> <span class="n">quad</span>

<span class="c1"># Estimate expected value of y by random sampling
</span><span class="n">y_mean1</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Estimate expected value of y using the result above 
# and numerical quadrature 
</span><span class="n">y_mean2</span> <span class="o">=</span> <span class="n">quad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">Px</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="o">-</span><span class="mf">6.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Random sampling estimate: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">y_mean1</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Integral estimate: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">y_mean2</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Random sampling estimate: 0.5014504701239816
Integral estimate: 0.49999999901341236
</code></pre></div></div>

<p>Given the importance of expected value integrals in filtering applications, considerable effort has gone into efficiently estimating Gaussian weighted integrals of the form shown in Equation \ref{eq:gwint}. For a simple 1D problem, a basic quadrature rule suffices. However, generalizations of 1D quadrature rules to many dimensions result in methods in which the number of points grows exponentially with dimension. Hence, these methods are computationally intractable for many problems. In the <a href="/sigmapy/examples/gaussian_quadrature/">next section</a>, we’ll show an example of a much more efficient method for computing Gaussian weighted integrals called the Unscented Transform.</p>

<h2 id="bibliography">Bibliography</h2>

<ol class="bibliography"><li><span id="Sarkka2013">Sarkka, S. (2013). Bayesian Filtering and Smoothing. <i>Cambridge University Press</i>. https://doi.org/10.1017/CBO9781139344203</span></li></ol>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/notebooks/2019/06/10/gaussian_quadrature/">
        Gaussian Quadrature
      </a>
    </h1>

    <span class="post-date">10 Jun 2019</span>

    <p>Here we consider the problem of efficiently estimating expected value integrals for a nonlinear function $f(\pmb{x}) : \mathbb{R}^n \to \mathbb{R}^m$
\begin{equation}
E[f(\pmb{x})] = \int_{\mathbb{R}^n} f(\pmb{x}) N(\pmb{x} | \pmb{0}, I) d \pmb{x}.
\end{equation}
Here the notation $N(\pmb{x} | \pmb{0}, I)$ is shorthand for the Gaussian probability density function with mean $\pmb{0}$ and identity covariance matrix, evaluated at the point $\pmb{x}$. In Bayesian filtering applications, the Gaussian weighting function is also referred to as the prior distribution. General Gaussian weight functions $N(\pmb{x} | \pmb{x_0}, P_x)$ are handled by performing a change of variables
\begin{equation}
\int_{\mathbb{R}^n} f(\pmb{x}) N(\pmb{x} | \pmb{x_0}, P_x) d \pmb{x} = \int f(\pmb{x_0} + \sqrt{P_x} \chi) N(\pmb{\chi} | \pmb{0}, I) d \pmb{x}
\end{equation}
where $\sqrt{P_x}$ is a matrix square root (typically obtained by Cholesky factorization) of the covariance matrix $P_x$. We will look at quadrature rules of the form 
\begin{equation}
 E[f(\pmb{x})] \approx Q[f(\pmb{x})] = \sum_i w_i f(\pmb{\chi_i})
\end{equation}
with quadrature points $\pmb{\chi_i}$, also called sigma points, and associated weights $w_i$.</p>

<h2 id="classifying-accuracy">Classifying Accuracy</h2>

<p>The order of accuracy of a Gaussian quadrature rule is usually classified in terms of the degrees of polynomial functions that it can integrate exactly. For multivariate functions we need a few definitions. A monomial of degree $d$ refers to a function 
\begin{equation}
x_1^{N_1} x_2^{N_2} \cdots x_n^{N_n}
\end{equation}
where the $N_i$ are non-negative integers that sum to $d$. A multivariate polynomial of degree $d$ is simply a weighted sum of monomial functions with highest degree $d$. A Gaussian quadrature is said to be $d$-th order if it can exactly integrate expectation integrals for polynomial functions $f(\pmb{x})$ up to and including degree $d$.</p>

<h2 id="gauss-hermite-quadrature">Gauss-Hermite Quadrature</h2>

<p>A standard approach to generate quadrature in many dimensions involves taking tensor products of 1D quadrature rules. In 1D, the third order Gauss-Hermite rule is given by 
\begin{equation}
\int_{\mathbb{R}} f(x) N(x | 0, 1) \; dx \approx \frac{2}{3} f \left ( 0 \right ) + \frac{1}{6} f \left ( -\sqrt{3} \right ) + \frac{1}{6} f \left ( \sqrt{3} \right )
\end{equation}</p>

<p>In $n$ dimensions, sigma points for the third order Gauss-Hermite rule are generated by taking tensor products of the three 1D sigma points. This yields a grid of $3^n$ points. Below we show an example in 3 dimensions. The size of each point corresponds to its weight.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sigmapy.sigma_sets</span> <span class="kn">import</span> <span class="n">SigmaSets</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s">'font.size'</span><span class="p">:</span> <span class="mi">18</span><span class="p">})</span>

<span class="c1"># Prior mean
</span><span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># Prior covariance
</span><span class="n">Px</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># Gauss-Hermite sigma points
</span><span class="n">sets</span> <span class="o">=</span> <span class="n">SigmaSets</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">wc</span> <span class="o">=</span> <span class="n">sets</span><span class="o">.</span><span class="n">get_set</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">Px</span><span class="p">,</span> <span class="n">set_name</span> <span class="o">=</span> <span class="s">'hermite'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Gauss-Hermite Sigma Points'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">s</span> <span class="o">=</span> <span class="mf">250.</span><span class="o">*</span><span class="n">wm</span> <span class="o">/</span> <span class="n">wm</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/gaussian_quadrature_files/gaussian_quadrature_1_0.png" alt="png" /></p>

<p>There is a clear drawback of the Gauss-Hermite quadrature rule. The number of sigma points grows exponentially with the state dimension! Hence, while the Gauss-Hermite rule is a useful for low dimensional problems, it is intractable for larger problems.</p>

<h2 id="the-unscented-transform-as-a-quadrature-rule">The Unscented Transform as a Quadrature Rule</h2>
<p>The unscented transform, introduced in <a class="citation" href="#Julier1997">(Julier &amp; Uhlmann, 1997)</a>, is a highly efficient quadrature rule of degree $d=3$. There are several formulations of the unscented transform, but we’ll look at a common version that uses set of $2n + 1$ sigma points. Here, the notation $\pmb{e_i}$ refers to the $i$-th column of the $n \times n$ identity matrix. Points are given by
\begin{equation}
\pmb{\chi_i} =
\begin{cases} 
      \pmb{x_0} &amp; i = 0 \\
      \pmb{x_0} - \sqrt{n + \kappa} \pmb{e_i} &amp; i = 1, \cdots, n\\
      \pmb{x_0} + \sqrt{n + \kappa} \pmb{e_i} &amp; i = n+1, \cdots, 2n
\end{cases}
\end{equation}
with weights
\begin{equation}
w_i^m = w_i^c =
\begin{cases} 
      \frac{\kappa}{n + \kappa} &amp; i = 0 \\
      \frac{1}{2(n+\kappa)} &amp; i = 1, \cdots, 2n.
\end{cases}
\end{equation}
Let’s look at a particular example. Suppose that 
\begin{equation}
f([x_0, x_1, x_2]) = [x_1 x_2 + 1, \: x_2^2, \: 5 x_0 x_1 x_2 + 2 x_1]
\end{equation}
Below we compute the expected value $ E[f(\pmb{x})]$ using both random sampling and the unscented transform using the Julier sigma points.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>

<span class="c1"># Nonlinear function
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">,:]</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">,:]</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mf">5.</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">,:]</span> <span class="o">+</span> <span class="mf">2.</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]])</span>

<span class="c1"># Estimate expected value via random sampling
</span><span class="n">samples</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="mi">5900</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">y_mean1</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Estimate using Gauss-Hermite
</span><span class="n">X</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">wc</span> <span class="o">=</span> <span class="n">sets</span><span class="o">.</span><span class="n">get_set</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">Px</span><span class="p">,</span> <span class="n">set_name</span> <span class="o">=</span> <span class="s">'hermite'</span><span class="p">)</span>
<span class="n">y_mean2</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">wm</span>

<span class="c1"># Estimate using UT
</span><span class="n">X</span><span class="p">,</span> <span class="n">wm</span><span class="p">,</span> <span class="n">wc</span> <span class="o">=</span> <span class="n">sets</span><span class="o">.</span><span class="n">get_set</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">Px</span><span class="p">,</span> <span class="n">set_name</span> <span class="o">=</span> <span class="s">'julier'</span><span class="p">,</span> <span class="n">kappa</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">)</span>
<span class="n">y_mean3</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">wm</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Expected value from random sampling: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">y_mean1</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Expected value from Gauss-Hermite: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">y_mean2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Expected value from UT: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">y_mean3</span><span class="p">))</span>


<span class="c1"># Plot UT sigmas 
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'UT Sigma Points'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">s</span> <span class="o">=</span> <span class="mf">250.</span><span class="o">*</span><span class="n">wm</span> <span class="o">/</span> <span class="n">wm</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Expected value from random sampling: [ 1.01513978  0.9864849  -0.00937191]
Expected value from Gauss-Hermite: [1. 1. 0.]
Expected value from UT: [1. 1. 0.]
</code></pre></div></div>

<p><img src="/assets/images/gaussian_quadrature_files/gaussian_quadrature_3_1.png" alt="png" /></p>

<p>In this case, both the Gauss-Hermite method and the UT exactly (to machine precision) compute the expected value integral since the $f(\pmb{x})$ has terms involving polynomials of at most degree 3. However, the UT only needs 7 sigma points versus 27 for Gauss-Hermite. Nice!</p>

<h2 id="higher-order-terms-and-integration-error">Higher Order Terms and Integration Error</h2>

<p>What about higher order polynomials or non-polynomial functions? A drawback of using a fixed number of sigma points versus taking random samples via MCMC methods is that the order of accuracy is necessarily fixed. For some nonlinear functions, the unscented approximation will be highly accurate (or even perfect), but for others, higher order polynomial terms in the Taylor series expansion for $f$ will result in integration errors. In contrast, MCMC methods can compute expectation integrals to arbitrary precision given enough samples.</p>

<p>Generally, the unscented transform provides more accurate estimates than MCMC methods relative to the number of sigma points / samples respectively. Different quadrature rules and different selections of the scaling parameters for those rules will affect their performance for a given problem. Sigmapy includes algorithms for generating a variety sigma point sets.</p>

<h2 id="bibliography">Bibliography</h2>

<ol class="bibliography"><li><span id="Julier1997">Julier, S. J., &amp; Uhlmann, J. K. (1997). New extension of the Kalman filter to nonlinear systems. <i>Int Symp AerospaceDefense Sensing Simul and Controls</i>. https://doi.org/10.1117/12.280797</span></li></ol>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/notebooks/2019/06/10/conditional_distributions/">
        Conditional Distributions
      </a>
    </h1>

    <span class="post-date">10 Jun 2019</span>

    <p>Gaussian filters use statistical linearization to obtain mean and covariance estimates. Suppose that $x \sim N(\pmb{x_0}, P_x)$ and
\begin{equation}
\pmb{y} = f(\pmb{x}) 
\end{equation}
where $f : \mathbb{R}^n \to \mathbb{R}^m$ is a nonlinear function. According to <a class="citation" href="#Sarkka2013">(Sarkka, 2013)</a>, in statistical linearization, one forms a linear approximation of the nonlinear transformation 
\begin{equation}
g(\pmb{x}) \approx \pmb{b} + A \delta \pmb{x} = \pmb{b} + A (\pmb{x} - \pmb{x_0})
\end{equation}
that minimizes the mean square error
\begin{equation}
\text{MSE}(\pmb{b}, A) = E[(f(\pmb{x}) - \pmb{b} -  A \delta \pmb{x})\; (f(\pmb{x}) - \pmb{b} -  A \delta \pmb{x})^T].
\end{equation}
Setting derivatives with respect to $A$ and $\pmb{b}$ yields
\begin{equation}
\pmb{b} = E[f(\pmb{x})]
\end{equation}
\begin{equation}
A = E[f(\pmb{x}) \delta \pmb{x}] P_x^{-1}.
\end{equation}
Here  $\pmb{b} = E[f(\pmb{x})]$ is exactly the mean of $\pmb{y} = f(\pmb{x})$. The approximate covariance is given by 
\begin{equation}
\begin{gathered}
P_y = E[(f(\pmb{x}) - E(f(\pmb{x})) \; (f(\pmb{x}) - E(f(\pmb{x}))^T] \\ 
\approx A P_x A^T = E[f(\pmb{x}) \; \delta \pmb{x}^T] P_x^{-1} E[f(\pmb{x}) \; \delta \pmb{x}^T].
\end{gathered}
\end{equation}
Note that statistical linearization involves computing expectation integrals $E[f(\pmb{x})]$ and $E[f(\pmb{x}) \; \delta \pmb{x}^T]$ of the form discussed in the sections on <a href="/notebooks/2019/06/10/gaussian_quadrature/">Gaussian quadrature</a>.</p>

<h2 id="conditional-distribution-for-an-additive-transform">Conditional Distribution for an Additive Transform</h2>

<p>Consider a noisy additive transform of the form 
\begin{equation}
\begin{gathered}
\pmb{y} = f(\pmb{x}) + \pmb{q} \\
x \sim N(\pmb{x_0}, P_x) \\
q \sim N(\pmb{0}, Q)
\end{gathered}
\end{equation}
where $\pmb{q}$ is the measurement noise. Suppose we have some observation $\pmb{y_0}$ and want to compute the mean and covariance of the probability distribution 
\begin{equation}
P( \pmb{x} | \pmb{y_0}).
\end{equation}
Performing statistical linearization on the augmented function $\hat{f}(\pmb{x}) = [\pmb{x}, f(\pmb{x})]$ yields the following approximations for mean and covariance
\begin{equation}
E[\hat{f}(\pmb{x})] \approx
\begin{bmatrix}
\pmb{x_0} \\
E[f(\pmb{x})]
\end{bmatrix}
=
\begin{bmatrix}
\pmb{x_0} \\
\pmb{\mu}
\end{bmatrix}
\end{equation}</p>

<p>\begin{equation}
\begin{gathered}
\text{Cov}[\hat{f}(\pmb{x})] \approx
\begin{bmatrix}
P_x &amp; E[f(\pmb{x}) \; \delta \pmb{x}^T]^T \\
E[f(\pmb{x}) \; \delta \pmb{x}^T] &amp; E[f(\pmb{x}) \; \delta \pmb{x}^T] P_x^{-1} E[f(\pmb{x}) \; \delta \pmb{x}^T] + Q
\end{bmatrix} \\
= 
\begin{bmatrix}
 P_x &amp; C \\
 C^T &amp; S 
\end{bmatrix}
\end{gathered}
\end{equation}
Here, $\mu$ is the measurement mean, and $S$ and $C$ are the measurement covariance and cross covariance respectively. Stated otherwise, statistical linearization yields a Gaussian approximation of the joint distribution 
\begin{equation}
\begin{bmatrix}
\pmb{x} \\
\pmb{y}
\end{bmatrix}
\sim 
N \left ( \begin{bmatrix}
\pmb{x_0} \\
\pmb{\mu}
\end{bmatrix},  \begin{bmatrix}
 P_x &amp; C \\
 C^T &amp; S
\end{bmatrix}\right ).
\end{equation}
Given tge measurement $\pmb{y_0}$, the mean and covariance of $\pmb{x} | \pmb{y_0}$ can be computed from the joint distribution as follows
\begin{equation}
\pmb{x’}  \sim N \left ( \pmb{x_0} + K[\pmb{y_o} - \pmb{\mu}], \; P_x - K S K^T \right )
\end{equation}
where $K = CS^{-1}$ is the Kalman gain.</p>

<h2 id="bibliography">Bibliography</h2>
<ol class="bibliography"><li><span id="Sarkka2013">Sarkka, S. (2013). Bayesian Filtering and Smoothing. <i>Cambridge University Press</i>. https://doi.org/10.1017/CBO9781139344203</span></li></ol>


  </div>
  
</div>

<div class="pagination">
  
    <span class="pagination-item older">Older</span>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
